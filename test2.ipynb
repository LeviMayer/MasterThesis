{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/vint_train\")\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# models\n",
    "from vint_train.models.gnm.gnm import GNM\n",
    "from vint_train.models.vint.vint import ViNT\n",
    "\n",
    "from vint_train.models.vint.vit import ViT\n",
    "from vint_train.models.nomad.nomad import NoMaD, DenseNetwork\n",
    "from vint_train.models.nomad.nomad_vint import NoMaD_ViNT, replace_bn_with_gn\n",
    "from diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "from vint_train.data.data_utils import IMAGE_ASPECT_RATIO\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model_path: str,\n",
    "    config: dict,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> nn.Module:\n",
    "    \"\"\"Load a model from a checkpoint file (works with models trained on multiple GPUs)\"\"\"\n",
    "    model_type = config[\"model_type\"]\n",
    "    \n",
    "    if model_type == \"gnm\":\n",
    "        model = GNM(\n",
    "            config[\"context_size\"],\n",
    "            config[\"len_traj_pred\"],\n",
    "            config[\"learn_angle\"],\n",
    "            config[\"obs_encoding_size\"],\n",
    "            config[\"goal_encoding_size\"],\n",
    "        )\n",
    "    elif model_type == \"vint\":\n",
    "        model = ViNT(\n",
    "            context_size=config[\"context_size\"],\n",
    "            len_traj_pred=config[\"len_traj_pred\"],\n",
    "            learn_angle=config[\"learn_angle\"],\n",
    "            obs_encoder=config[\"obs_encoder\"],\n",
    "            obs_encoding_size=config[\"obs_encoding_size\"],\n",
    "            late_fusion=config[\"late_fusion\"],\n",
    "            mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "            mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "            mha_ff_dim_factor=config[\"mha_ff_dim_factor\"],\n",
    "        )\n",
    "    elif config[\"model_type\"] == \"nomad\":\n",
    "        if config[\"vision_encoder\"] == \"nomad_vint\":\n",
    "            vision_encoder = NoMaD_ViNT(\n",
    "                obs_encoding_size=config[\"encoding_size\"],\n",
    "                context_size=config[\"context_size\"],\n",
    "                mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "                mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "                mha_ff_dim_factor=config[\"mha_ff_dim_factor\"],\n",
    "            )\n",
    "            vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "        elif config[\"vision_encoder\"] == \"vit\": \n",
    "            vision_encoder = ViT(\n",
    "                obs_encoding_size=config[\"encoding_size\"],\n",
    "                context_size=config[\"context_size\"],\n",
    "                image_size=config[\"image_size\"],\n",
    "                patch_size=config[\"patch_size\"],\n",
    "                mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "                mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "            )\n",
    "            vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "        else: \n",
    "            raise ValueError(f\"Vision encoder {config['vision_encoder']} not supported\")\n",
    "        \n",
    "        noise_pred_net = ConditionalUnet1D(\n",
    "                input_dim=2,\n",
    "                global_cond_dim=config[\"encoding_size\"],\n",
    "                down_dims=config[\"down_dims\"],\n",
    "                cond_predict_scale=config[\"cond_predict_scale\"],\n",
    "            )\n",
    "        dist_pred_network = DenseNetwork(embedding_dim=config[\"encoding_size\"])\n",
    "        \n",
    "        model = NoMaD(\n",
    "            vision_encoder=vision_encoder,\n",
    "            noise_pred_net=noise_pred_net,\n",
    "            dist_pred_net=dist_pred_network,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if model_type == \"nomad\":\n",
    "        state_dict = checkpoint\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        loaded_model = checkpoint[\"model\"]\n",
    "        try:\n",
    "            state_dict = loaded_model.module.state_dict()\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        except AttributeError as e:\n",
    "            state_dict = loaded_model.state_dict()\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model configuration from C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/config/nomad.yaml\n",
      "Loading model from C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/model_weights/nomad.pth\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Assuming load_model is a utility function available in the utils module\u001b[39;00m\n\u001b[0;32m     34\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/src\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(ckpth_path, model_params, device)\n",
      "File \u001b[1;32mC:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/src\\utils1.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/vint_train/diffusion_policy_main/diffusion_policy-main\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# models\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#from vint_train.models.gnm.gnm import GNM\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvint_train\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViNT\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvint_train\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgnm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgnm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GNM\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvint_train\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViT\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define the path to the YAML configuration files and model weights\n",
    "MODEL_WEIGHTS_PATH = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/model_weights\"\n",
    "MODEL_CONFIG_PATH = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/config/models.yaml\"\n",
    "\n",
    "# Load the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load model configuration from YAML\n",
    "with open(MODEL_CONFIG_PATH, \"r\") as f:\n",
    "    model_paths = yaml.safe_load(f)\n",
    "\n",
    "# Specify the model to load (assuming \"nomad\" in this case)\n",
    "model_name = \"nomad\"\n",
    "model_config_path = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/config/nomad.yaml\"\n",
    "print(f\"Loading model configuration from {model_config_path}\")\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    model_params = yaml.safe_load(f)\n",
    "\n",
    "# Load model weights from the checkpoint file\n",
    "ckpth_path = model_paths[model_name][\"ckpt_path\"]\n",
    "if os.path.exists(ckpth_path):\n",
    "    print(f\"Loading model from {ckpth_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model weights not found at {ckpth_path}\")\n",
    "\n",
    "# Assuming load_model is a utility function available in the utils module\n",
    "sys.path.append((\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/src\"))\n",
    "from utils1 import load_model\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model(ckpth_path, model_params, device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 6, 3, 3], expected input[1, 3, 86, 65] to have 6 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m85\u001b[39m,\u001b[38;5;241m64\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m obsgoal_cond \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvision_encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgoal_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_goal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train\\vint_train\\models\\nomad\\nomad.py:24\u001b[0m, in \u001b[0;36mNoMaD.forward\u001b[1;34m(self, func_name, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, func_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m :\n\u001b[1;32m---> 24\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs_img\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoal_img\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_goal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_goal_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m func_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoise_pred_net\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     26\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_pred_net(sample\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m], timestep\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m], global_cond\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_cond\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train\\vint_train\\models\\nomad\\nomad_vint.py:85\u001b[0m, in \u001b[0;36mNoMaD_ViNT.forward\u001b[1;34m(self, obs_img, goal_img, input_goal_mask)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Get the goal encoding\u001b[39;00m\n\u001b[0;32m     84\u001b[0m obsgoal_img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([obs_img[:, \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_size:, :, :], goal_img], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# concatenate the obs image/context and goal image --> non image goal?\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m obsgoal_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobsgoal_img\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get encoding of this img \u001b[39;00m\n\u001b[0;32m     86\u001b[0m obsgoal_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_encoder\u001b[38;5;241m.\u001b[39m_avg_pooling(obsgoal_encoding) \u001b[38;5;66;03m# avg pooling \u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_encoder\u001b[38;5;241m.\u001b[39m_global_params\u001b[38;5;241m.\u001b[39minclude_top:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\efficientnet_pytorch\\model.py:289\u001b[0m, in \u001b[0;36mEfficientNet.extract_features\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"use convolution layer to extract feature .\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m    layer in the efficientnet model.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Stem\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn0(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# Blocks\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\efficientnet_pytorch\\utils.py:275\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_padding(x)\n\u001b[1;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 6, 3, 3], expected input[1, 3, 86, 65] to have 6 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "image = torch.zeros((1, 3,85,64))\n",
    "obsgoal_cond = model('vision_encoder', obs_img=image,goal_img = image, input_goal_mask= image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\levid\\anaconda3\\envs\\nomad_train\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(MODEL_CONFIG_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     30\u001b[0m     model_paths \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m---> 32\u001b[0m model_config_path \u001b[38;5;241m=\u001b[39m model_paths[\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mmodel][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_config_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     34\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "from vint_train.models.gnm.gnm import GNM\n",
    "from vint_train.models.nomad.nomad import DenseNetwork, NoMaD\n",
    "from vint_train.models.nomad.nomad_vint import NoMaD_ViNT, replace_bn_with_gn\n",
    "from vint_train.models.vint.vint import ViNT\n",
    "from vint_train.models.vint.vit import ViT\n",
    "\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the YAML configuration files and model weights\n",
    "MODEL_WEIGHTS_PATH = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/model_weights\"\n",
    "MODEL_CONFIG_PATH = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/config/models.yaml\"\n",
    "\n",
    "# Load the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load model configuration from YAML\n",
    "with open(MODEL_CONFIG_PATH, \"r\") as f:\n",
    "    model_paths = yaml.safe_load(f)\n",
    "\n",
    "# Specify the model to load (assuming \"nomad\" in this case)\n",
    "model_name = \"nomad\"\n",
    "model_config_path = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/config/nomad.yaml\"\n",
    "print(f\"Loading model configuration from {model_config_path}\")\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    model_params = yaml.safe_load(f)\n",
    "\n",
    "# Load model weights from the checkpoint file\n",
    "ckpth_path = model_paths[model_name][\"ckpt_path\"]\n",
    "if os.path.exists(ckpth_path):\n",
    "    print(f\"Loading model from {ckpth_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model weights not found at {ckpth_path}\")\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model_path: str,\n",
    "    config: dict,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a model from a checkpoint file (works with models trained on multiple GPUs)\"\"\"\n",
    "    model_type = config[\"model_type\"]\n",
    "    \n",
    "    if model_type == \"gnm\":\n",
    "        model = GNM(\n",
    "            config[\"context_size\"],\n",
    "            config[\"len_traj_pred\"],\n",
    "            config[\"learn_angle\"],\n",
    "            config[\"obs_encoding_size\"],\n",
    "            config[\"goal_encoding_size\"],\n",
    "        )\n",
    "    elif model_type == \"vint\":\n",
    "        model = ViNT(\n",
    "            context_size=config[\"context_size\"],\n",
    "            len_traj_pred=config[\"len_traj_pred\"],\n",
    "            learn_angle=config[\"learn_angle\"],\n",
    "            obs_encoder=config[\"obs_encoder\"],\n",
    "            obs_encoding_size=config[\"obs_encoding_size\"],\n",
    "            late_fusion=config[\"late_fusion\"],\n",
    "            mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "            mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "            mha_ff_dim_factor=config[\"mha_ff_dim_factor\"],\n",
    "        )\n",
    "    elif config[\"model_type\"] == \"nomad\":\n",
    "        if config[\"vision_encoder\"] == \"nomad_vint\":\n",
    "            vision_encoder = NoMaD_ViNT(\n",
    "                obs_encoding_size=config[\"encoding_size\"],\n",
    "                context_size=config[\"context_size\"],\n",
    "                mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "                mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "                mha_ff_dim_factor=config[\"mha_ff_dim_factor\"],\n",
    "            )\n",
    "            vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "        elif config[\"vision_encoder\"] == \"vit\": \n",
    "            vision_encoder = ViT(\n",
    "                obs_encoding_size=config[\"encoding_size\"],\n",
    "                context_size=config[\"context_size\"],\n",
    "                image_size=config[\"image_size\"],\n",
    "                patch_size=config[\"patch_size\"],\n",
    "                mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "                mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "            )\n",
    "            vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "        else: \n",
    "            raise ValueError(f\"Vision encoder {config['vision_encoder']} not supported\")\n",
    "        \n",
    "        noise_pred_net = ConditionalUnet1D(\n",
    "                input_dim=2,\n",
    "                global_cond_dim=config[\"encoding_size\"],\n",
    "                down_dims=config[\"down_dims\"],\n",
    "                cond_predict_scale=config[\"cond_predict_scale\"],\n",
    "            )\n",
    "        dist_pred_network = DenseNetwork(embedding_dim=config[\"encoding_size\"])\n",
    "        \n",
    "        model = NoMaD(\n",
    "            vision_encoder=vision_encoder,\n",
    "            noise_pred_net=noise_pred_net,\n",
    "            dist_pred_net=dist_pred_network,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if model_type == \"nomad\":\n",
    "        state_dict = checkpoint\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        loaded_model = checkpoint[\"model\"]\n",
    "        try:\n",
    "            state_dict = loaded_model.module.state_dict()\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        except AttributeError as e:\n",
    "            state_dict = loaded_model.state_dict()\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model(ckpth_path, model_params, device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((85, 64)),      # Resize to 85x64\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])         # Convert to a tensor with shape (3, 85, 64)\n",
    "])\n",
    "image1 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image10.png\").convert(\"RGB\"))\n",
    "image2 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image20.png\").convert(\"RGB\"))\n",
    "image3 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image30.png\").convert(\"RGB\"))\n",
    "image4 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image40.png\").convert(\"RGB\"))\n",
    "image = torch.stack([image1,image2,image3,image4], dim=0).permute(1, 0, 2, 3).reshape(1, 12, 85, 64)\n",
    "\n",
    "#image = torch.zeros((1, 12,85,64))\n",
    "goal = torch.zeros((1, 3,85,64))\n",
    "obsgoal_cond = model('vision_encoder', obs_img=image,goal_img = goal, input_goal_mask= None)\n",
    "dists = model(\"dist_pred_net\", obsgoal_cond=obsgoal_cond)\n",
    "#infere Action\n",
    "noise_scheduler = DDPMScheduler(\n",
    "            num_train_timesteps=model_params[\"num_diffusion_iters\"],\n",
    "            beta_schedule='squaredcos_cap_v2',\n",
    "            clip_sample=True,\n",
    "            prediction_type='epsilon'\n",
    "        )\n",
    "obs_cond = obsgoal_cond\n",
    "num_samples = 1\n",
    "with torch.no_grad():\n",
    "    # encoder vision features\n",
    "    if len(obs_cond.shape) == 2:\n",
    "        obs_cond = obs_cond.repeat(num_samples, 1)\n",
    "    else:\n",
    "        obs_cond = obs_cond.repeat(num_samples, 1, 1)\n",
    "    \n",
    "    # initialize action from Gaussian noise\n",
    "    noisy_action = torch.randn(\n",
    "        (num_samples, model_params[\"len_traj_pred\"], 2), device=device)\n",
    "    naction = noisy_action\n",
    "\n",
    "    # init scheduler\n",
    "    noise_scheduler.set_timesteps(model_params[\"num_diffusion_iters\"])\n",
    "\n",
    "    start_time = time.time()\n",
    "    for k in noise_scheduler.timesteps[:]:\n",
    "        # predict noise\n",
    "        noise_pred = model(\n",
    "            'noise_pred_net',\n",
    "            sample=naction,\n",
    "            timestep=k,\n",
    "            global_cond=obs_cond\n",
    "        )\n",
    "        # inverse diffusion step (remove noise)\n",
    "        naction = noise_scheduler.step(\n",
    "            model_output=noise_pred,\n",
    "            timestep=k,\n",
    "            sample=naction\n",
    "        ).prev_sample\n",
    "    print(\"time elapsed:\", time.time() - start_time)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(naction[0, :, 0].cpu().numpy(), naction[0, :, 1].cpu().numpy())\n",
    "plt.show()\n",
    "print(\"Model loaded successfully!\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model configuration from C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/config/nomad.yaml\n",
      "Loading model from C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/model_weights/nomad.pth\n",
      "time elapsed: 0.32105469703674316\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.8999707e-01 -2.2153854e-03]\n",
      "  [ 8.0978990e-01  1.8138885e-02]\n",
      "  [ 1.3135797e+00  6.3529015e-02]\n",
      "  [ 1.9222124e+00  1.5610027e-01]\n",
      "  [ 2.4856610e+00  3.3504343e-01]\n",
      "  [ 3.1018729e+00  6.1886692e-01]\n",
      "  [ 3.8647082e+00  1.0294118e+00]\n",
      "  [ 4.6364422e+00  1.4260612e+00]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "NoMaD(\n",
      "  (vision_encoder): NoMaD_ViNT(\n",
      "    (obs_encoder): EfficientNet(\n",
      "      (_conv_stem): Conv2dStaticSamePadding(\n",
      "        3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
      "      )\n",
      "      (_bn0): GroupNorm(2, 32, eps=1e-05, affine=True)\n",
      "      (_blocks): ModuleList(\n",
      "        (0): MBConvBlock(\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(2, 32, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (1): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(6, 96, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(6, 96, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (2): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (3): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(2, 40, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (4): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(2, 40, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (5): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(5, 80, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (6-7): 2 x MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(5, 80, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (8): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(7, 112, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (9-10): 2 x MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(7, 112, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (11): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(12, 192, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (12-14): 3 x MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(12, 192, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (15): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(20, 320, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "      )\n",
      "      (_conv_head): Conv2dStaticSamePadding(\n",
      "        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn1): GroupNorm(80, 1280, eps=1e-05, affine=True)\n",
      "      (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "      (_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (goal_encoder): EfficientNet(\n",
      "      (_conv_stem): Conv2dStaticSamePadding(\n",
      "        6, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
      "      )\n",
      "      (_bn0): GroupNorm(2, 32, eps=1e-05, affine=True)\n",
      "      (_blocks): ModuleList(\n",
      "        (0): MBConvBlock(\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(2, 32, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (1): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(6, 96, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(6, 96, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (2): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(1, 24, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (3): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(9, 144, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(2, 40, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (4): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(2, 40, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (5): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "            (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(15, 240, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(5, 80, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (6-7): 2 x MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(5, 80, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (8): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(30, 480, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(7, 112, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (9-10): 2 x MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(7, 112, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (11): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(42, 672, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(12, 192, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (12-14): 3 x MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "            (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
      "          )\n",
      "          (_bn1): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(12, 192, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "        (15): MBConvBlock(\n",
      "          (_expand_conv): Conv2dStaticSamePadding(\n",
      "            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn0): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "            1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "            (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
      "          )\n",
      "          (_bn1): GroupNorm(72, 1152, eps=1e-05, affine=True)\n",
      "          (_se_reduce): Conv2dStaticSamePadding(\n",
      "            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_se_expand): Conv2dStaticSamePadding(\n",
      "            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_project_conv): Conv2dStaticSamePadding(\n",
      "            1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (static_padding): Identity()\n",
      "          )\n",
      "          (_bn2): GroupNorm(20, 320, eps=1e-05, affine=True)\n",
      "          (_swish): MemoryEfficientSwish()\n",
      "        )\n",
      "      )\n",
      "      (_conv_head): Conv2dStaticSamePadding(\n",
      "        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn1): GroupNorm(80, 1280, eps=1e-05, affine=True)\n",
      "      (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "      (_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (compress_obs_enc): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (compress_goal_enc): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (sa_layer): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (sa_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (noise_pred_net): ConditionalUnet1D(\n",
      "    (mid_modules): ModuleList(\n",
      "      (0-1): 2 x ConditionalResidualBlock1D(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x Conv1dBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "              (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "              (2): Mish()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (cond_encoder): Sequential(\n",
      "          (0): Mish()\n",
      "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (2): Rearrange('batch t -> batch t 1')\n",
      "        )\n",
      "        (residual_conv): Identity()\n",
      "      )\n",
      "    )\n",
      "    (diffusion_step_encoder): Sequential(\n",
      "      (0): SinusoidalPosEmb()\n",
      "      (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (2): Mish()\n",
      "      (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "    (up_modules): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(512, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Identity()\n",
      "        )\n",
      "        (2): Upsample1d(\n",
      "          (conv): ConvTranspose1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(256, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=64, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=64, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Identity()\n",
      "        )\n",
      "        (2): Upsample1d(\n",
      "          (conv): ConvTranspose1d(64, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (down_modules): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=64, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Conv1d(2, 64, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=64, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Identity()\n",
      "        )\n",
      "        (2): Downsample1d(\n",
      "          (conv): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Identity()\n",
      "        )\n",
      "        (2): Downsample1d(\n",
      "          (conv): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConditionalResidualBlock1D(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (cond_encoder): Sequential(\n",
      "            (0): Mish()\n",
      "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (2): Rearrange('batch t -> batch t 1')\n",
      "          )\n",
      "          (residual_conv): Identity()\n",
      "        )\n",
      "        (2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (final_conv): Sequential(\n",
      "      (0): Conv1dBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "          (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (2): Mish()\n",
      "        )\n",
      "      )\n",
      "      (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (dist_pred_net): DenseNetwork(\n",
      "    (network): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from std_msgs.msg import Bool, Float32MultiArray\n",
    "from diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "from vint_train.models.gnm.gnm import GNM\n",
    "from vint_train.models.nomad.nomad import DenseNetwork, NoMaD\n",
    "from vint_train.models.nomad.nomad_vint import NoMaD_ViNT, replace_bn_with_gn\n",
    "from vint_train.models.vint.vint import ViNT\n",
    "from vint_train.models.vint.vit import ViT\n",
    "from vint_train.training.train_utils import get_action\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the YAML configuration files and model weights\n",
    "MODEL_WEIGHTS_PATH = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/model_weights\"\n",
    "MODEL_CONFIG_PATH = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/deployment/config/models.yaml\"\n",
    "\n",
    "# Load the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load model configuration from YAML\n",
    "with open(MODEL_CONFIG_PATH, \"r\") as f:\n",
    "    model_paths = yaml.safe_load(f)\n",
    "\n",
    "# Specify the model to load (assuming \"nomad\" in this case)\n",
    "model_name = \"nomad\"\n",
    "model_config_path = \"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/model/visualnav-transformer-main/train/config/nomad.yaml\"\n",
    "print(f\"Loading model configuration from {model_config_path}\")\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    model_params = yaml.safe_load(f)\n",
    "\n",
    "# Load model weights from the checkpoint file\n",
    "ckpth_path = model_paths[model_name][\"ckpt_path\"]\n",
    "if os.path.exists(ckpth_path):\n",
    "    print(f\"Loading model from {ckpth_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model weights not found at {ckpth_path}\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "def load_model(\n",
    "    model_path: str,\n",
    "    config: dict,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a model from a checkpoint file (works with models trained on multiple GPUs)\"\"\"\n",
    "    model_type = config[\"model_type\"]\n",
    "    \n",
    "    if model_type == \"gnm\":\n",
    "        model = GNM(\n",
    "            config[\"context_size\"],\n",
    "            config[\"len_traj_pred\"],\n",
    "            config[\"learn_angle\"],\n",
    "            config[\"obs_encoding_size\"],\n",
    "            config[\"goal_encoding_size\"],\n",
    "        )\n",
    "    elif model_type == \"vint\":\n",
    "        model = ViNT(\n",
    "            context_size=config[\"context_size\"],\n",
    "            len_traj_pred=config[\"len_traj_pred\"],\n",
    "            learn_angle=config[\"learn_angle\"],\n",
    "            obs_encoder=config[\"obs_encoder\"],\n",
    "            obs_encoding_size=config[\"obs_encoding_size\"],\n",
    "            late_fusion=config[\"late_fusion\"],\n",
    "            mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "            mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "            mha_ff_dim_factor=config[\"mha_ff_dim_factor\"],\n",
    "        )\n",
    "    elif config[\"model_type\"] == \"nomad\":\n",
    "        if config[\"vision_encoder\"] == \"nomad_vint\":\n",
    "            vision_encoder = NoMaD_ViNT(\n",
    "                obs_encoding_size=config[\"encoding_size\"],\n",
    "                context_size=config[\"context_size\"],\n",
    "                mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "                mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "                mha_ff_dim_factor=config[\"mha_ff_dim_factor\"],\n",
    "            )\n",
    "            vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "        elif config[\"vision_encoder\"] == \"vit\": \n",
    "            vision_encoder = ViT(\n",
    "                obs_encoding_size=config[\"encoding_size\"],\n",
    "                context_size=config[\"context_size\"],\n",
    "                image_size=config[\"image_size\"],\n",
    "                patch_size=config[\"patch_size\"],\n",
    "                mha_num_attention_heads=config[\"mha_num_attention_heads\"],\n",
    "                mha_num_attention_layers=config[\"mha_num_attention_layers\"],\n",
    "            )\n",
    "            vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "        else: \n",
    "            raise ValueError(f\"Vision encoder {config['vision_encoder']} not supported\")\n",
    "        \n",
    "        noise_pred_net = ConditionalUnet1D(\n",
    "                input_dim=2,\n",
    "                global_cond_dim=config[\"encoding_size\"],\n",
    "                down_dims=config[\"down_dims\"],\n",
    "                cond_predict_scale=config[\"cond_predict_scale\"],\n",
    "            )\n",
    "        dist_pred_network = DenseNetwork(embedding_dim=config[\"encoding_size\"])\n",
    "        \n",
    "        model = NoMaD(\n",
    "            vision_encoder=vision_encoder,\n",
    "            noise_pred_net=noise_pred_net,\n",
    "            dist_pred_net=dist_pred_network,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if model_type == \"nomad\":\n",
    "        state_dict = checkpoint\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        loaded_model = checkpoint[\"model\"]\n",
    "        try:\n",
    "            state_dict = loaded_model.module.state_dict()\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "        except AttributeError as e:\n",
    "            state_dict = loaded_model.state_dict()\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model(ckpth_path, model_params, device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((85, 64)),      # Resize to 85x64\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])              # Convert to a tensor with shape (3, 85, 64)\n",
    "])\n",
    "#image1 = transform(Image.open(\"../../go_stanford/go_stanford/no1vcF_7_1/93.jpg\").convert(\"RGB\"))\n",
    "#image2 = transform(Image.open(\"../../go_stanford/go_stanford/no1vcF_7_1/94.jpg\").convert(\"RGB\"))\n",
    "#image3 = transform(Image.open(\"../../go_stanford/go_stanford/no1vcF_7_1/95.jpg\").convert(\"RGB\"))\n",
    "#image4 = transform(Image.open(\"../../go_stanford/go_stanford/no1vcF_7_1/96.jpg\").convert(\"RGB\"))\n",
    "image1 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image350.png\").convert(\"RGB\"))\n",
    "image2 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image360.png\").convert(\"RGB\"))\n",
    "image3 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image370.png\").convert(\"RGB\"))\n",
    "image4 = transform(Image.open(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image380.png\").convert(\"RGB\"))\n",
    "image = torch.stack([image1,image2,image3,image4], dim=0).permute(1, 0, 2, 3).reshape(1, 12, 85, 64)\n",
    "\n",
    "#image = torch.zeros((1, 12,85,64))\n",
    "goal = torch.zeros((1, 3,85,64))\n",
    "obsgoal_cond = model('vision_encoder', obs_img=image,goal_img = goal, input_goal_mask= None)\n",
    "dists = model(\"dist_pred_net\", obsgoal_cond=obsgoal_cond)\n",
    "#infere Action\n",
    "noise_scheduler = DDPMScheduler(\n",
    "            num_train_timesteps=model_params[\"num_diffusion_iters\"],\n",
    "            beta_schedule='squaredcos_cap_v2',\n",
    "            clip_sample=True,\n",
    "            prediction_type='epsilon'\n",
    "        )\n",
    "obs_cond = obsgoal_cond\n",
    "num_samples = 1\n",
    "with torch.no_grad():\n",
    "    # encoder vision features\n",
    "    if len(obs_cond.shape) == 2:\n",
    "        obs_cond = obs_cond.repeat(num_samples, 1)\n",
    "    else:\n",
    "        obs_cond = obs_cond.repeat(num_samples, 1, 1)\n",
    "    \n",
    "    # initialize action from Gaussian noise\n",
    "    noisy_action = torch.randn(\n",
    "        (num_samples, model_params[\"len_traj_pred\"], 2), device=device)\n",
    "    naction = noisy_action\n",
    "\n",
    "    # init scheduler\n",
    "    noise_scheduler.set_timesteps(model_params[\"num_diffusion_iters\"])\n",
    "\n",
    "    start_time = time.time()\n",
    "    for k in noise_scheduler.timesteps[:]:\n",
    "        # predict noise\n",
    "        noise_pred = model(\n",
    "            'noise_pred_net',\n",
    "            sample=naction,\n",
    "            timestep=k,\n",
    "            global_cond=obs_cond\n",
    "        )\n",
    "        # inverse diffusion step (remove noise)\n",
    "        naction = noise_scheduler.step(\n",
    "            model_output=noise_pred,\n",
    "            timestep=k,\n",
    "            sample=naction\n",
    "        ).prev_sample\n",
    "    print(\"time elapsed:\", time.time() - start_time)\n",
    "naction2 = to_numpy(get_action(naction))\n",
    "sampled_actions_msg = Float32MultiArray()\n",
    "sampled_actions_msg.data = np.concatenate((np.array([0]), naction2.flatten()))\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(naction[0, :, 0].cpu().numpy(), naction[0, :, 1].cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "print(naction2)\n",
    "\n",
    "plt.scatter(naction2[:, 0], naction2[:, 1])\n",
    "plt.show()\n",
    "print(\"Model loaded successfully!\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0520, -0.0041],\n",
      "         [-0.0654, -0.0027],\n",
      "         [-0.0866,  0.0022],\n",
      "         [-0.0996,  0.0090],\n",
      "         [-0.0714,  0.0078],\n",
      "         [-0.0813,  0.0236],\n",
      "         [-0.0848,  0.0532],\n",
      "         [-0.0905,  0.0659]]])\n",
      "[[[ 1.0548863  -0.01628208]\n",
      "  [ 2.0597267  -0.02714419]\n",
      "  [ 2.9849796  -0.01840138]\n",
      "  [ 3.8614616   0.01766801]\n",
      "  [ 4.8437643   0.04871869]\n",
      "  [ 5.788789    0.1429193 ]\n",
      "  [ 6.7206035   0.35579848]\n",
      "  [ 7.631062    0.61927676]]]\n",
      "(1, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "print(naction)\n",
    "print(naction2)\n",
    "print(naction2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Bild laden\n",
    "img = cv2.imread(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image390.png\")  # Ersetze 'dein_bild.jpg' durch den Pfad zu deinem Bild\n",
    "\n",
    "coordinates = naction2\n",
    "\n",
    "\n",
    "# Bilddimensionen und Ursprung (untere Mitte des Bildes)\n",
    "image_width, image_height = img.shape[1], img.shape[0]\n",
    "start_point = (image_width // 2, image_height//2)  # Untere Mitte\n",
    "\n",
    "# Transformiere Koordinaten relativ zur unteren Mitte\n",
    "absolute_coordinates = [start_point]  # Die erste Koordinate ist der Startpunkt\n",
    "for coord in coordinates[0]:\n",
    "    x = int(start_point[0] + coord[0] * image_width / 100)  # Skaliere x relativ zur Mitte\n",
    "    y = int(start_point[1] + coord[1] * image_height / 100)  # Skaliere y nach oben\n",
    "    absolute_coordinates.append((x, y))\n",
    "\n",
    "# Zeichne Linien zwischen den Koordinaten\n",
    "#for i in range(len(absolute_coordinates) - 1):\n",
    "    #start_point = absolute_coordinates[i]\n",
    "end_point = absolute_coordinates[- 1]\n",
    "cv2.line(img, start_point, end_point, (0, 255, 0), 2)  # Grne Linien\n",
    "\n",
    "\n",
    "# Bild hochskalieren\n",
    "upscale_factor = 4  # Faktor zur Vergrerung\n",
    "new_width = img.shape[1] * upscale_factor\n",
    "new_height = img.shape[0] * upscale_factor\n",
    "upscaled_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Bild anzeigen\n",
    "cv2.imshow(\"Pfeile auf Bild\", upscaled_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Bild laden\n",
    "img = cv2.imread(\"C:/Users/levid/Desktop/Uni/Master/SoSe24/ProjektLearningRobots/Webot_Pictures/1/camera_image10.png\")  # Ersetze 'dein_bild.jpg' durch den Pfad zu deinem Bild\n",
    "\n",
    "coordinates = naction2\n",
    "\n",
    "image_width, image_height = img.shape[1], img.shape[0]\n",
    "start_point = (image_width // 2, image_height)  # Untere Mitte\n",
    "\n",
    "# Transformiere Koordinaten relativ zur unteren Mitte\n",
    "absolute_coordinates = [start_point]  # Die erste Koordinate ist der Startpunkt\n",
    "for coord in coordinates[0]:\n",
    "    x = int(start_point[0] + coord[0] * image_width / 10)  # Skaliere x relativ zur Mitte\n",
    "    y = int(start_point[1] - coord[1] * image_height / 10)  # Skaliere y nach oben\n",
    "    absolute_coordinates.append((x, y))\n",
    "\n",
    "# Zeichne Linien (blau fr den Hauptpfad, gelb fr alternative Pfade)\n",
    "for i in range(len(absolute_coordinates) - 1):\n",
    "    start_point = absolute_coordinates[i]\n",
    "    end_point = absolute_coordinates[i + 1]\n",
    "    cv2.line(img, start_point, end_point, (255, 0, 0), 2)  # Blaue Linie\n",
    "\n",
    "# Zustzliche gelbe Linien (simuliert alternative Pfade)\n",
    "# Beispiel: leicht abweichende Koordinaten\n",
    "alternative_coordinates = [\n",
    "    (coord[0] + 0.5, coord[1] + 0.2) for coord in coordinates[0]\n",
    "]\n",
    "alternative_absolute = [\n",
    "    (int(start_point[0] + coord[0] * image_width / 10),\n",
    "     int(start_point[1] - coord[1] * image_height / 10))\n",
    "    for coord in alternative_coordinates\n",
    "]\n",
    "for i in range(len(alternative_absolute) - 1):\n",
    "    cv2.line(img, alternative_absolute[i], alternative_absolute[i + 1], (0, 255, 255), 2)\n",
    "\n",
    "# Bild hochskalieren\n",
    "upscale_factor = 4  # Faktor zur Vergrerung\n",
    "new_width = img.shape[1] * upscale_factor\n",
    "new_height = img.shape[0] * upscale_factor\n",
    "upscaled_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Bild anzeigen\n",
    "cv2.imshow(\"Pfeile auf Bild\", upscaled_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomad_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
